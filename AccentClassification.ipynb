{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from pydub import AudioSegment, silence\n",
    "from pydub.silence import split_on_silence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "              \n",
    "        self.mfccs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range(len(os.listdir(self.root_dir))):\n",
    "\n",
    "            f = os.listdir(self.root_dir)[i]\n",
    "            with open(self.root_dir / f, 'rb') as handle:\n",
    "                entry = pickle.load(handle)\n",
    "            self.mfccs.append(entry['data'])\n",
    "            self.labels.extend(entry['labels'])\n",
    "        \n",
    "        self.mfccs = torch.from_numpy(np.vstack(self.mfccs).reshape(-1,3,20,22)).float()\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.mfccs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.mfccs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_csv = Path('C:/Users/omar_/Documents/cockatoos/data/accent_samples/speakers_all.csv')\n",
    "speakers_df = pd.read_csv(speaker_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579\n",
      "198\n",
      "Number of english accent files: 579\n",
      "Number of other accent files: 198\n",
      "Number of training files: 400\n",
      "Number of validaiton files: 100\n",
      "Number of test files: 96\n"
     ]
    }
   ],
   "source": [
    "original_dir = Path('C:/Users/omar_/Documents/cockatoos/data/accent_samples/recordings/recordings')\n",
    "train_dir =  Path('C:/Users/omar_/Documents/cockatoos/data/train')\n",
    "val_dir =  Path('C:/Users/omar_/Documents/cockatoos/data/val')\n",
    "test_dir =  Path('C:/Users/omar_/Documents/cockatoos/data/test')\n",
    "\n",
    "files = os.listdir(original_dir)\n",
    "\n",
    "\n",
    "other_accent_types = [\"mandarin\", \"japanese\", \"korean\", \"taiwanese\", \"cantonese\", \"thai\", \"indonesian\"]\n",
    "\n",
    "english_accent_files = []\n",
    "other_accent_files = []\n",
    "\n",
    "num_train_files = 200\n",
    "num_val_files = 50\n",
    "num_test_files = 48\n",
    "\n",
    "# num_train_files = 50\n",
    "# num_val_files = 19\n",
    "# num_test_files = 19\n",
    "\n",
    "end_idx_train = num_train_files\n",
    "end_idx_val = end_idx_train + num_val_files\n",
    "end_idx_test = end_idx_val + num_test_files\n",
    "\n",
    "\n",
    "# for i in range(len(speakers_df)):\n",
    "#     if df.loc[i]['file_missing?'] == False:\n",
    "#         country = df.loc[i]['country']\n",
    "#         if country == \"usa\":\n",
    "#             english_accent_files.append(f\"{df.loc[i]['filename']}.mp3\")\n",
    "#         elif country == \"china\":\n",
    "#             other_accent_files.append(f\"{df.loc[i]['filename']}.mp3\")\n",
    "\n",
    "for f in files:\n",
    "    if \"english\" in f:\n",
    "        english_accent_files.append(f)\n",
    "    \n",
    "    if any(t in f for t in other_accent_types):\n",
    "        other_accent_files.append(f)\n",
    "\n",
    "\n",
    "print(len(english_accent_files))\n",
    "print(len(other_accent_files))\n",
    "\n",
    "np.random.seed(100)\n",
    "np.random.shuffle(english_accent_files)\n",
    "np.random.shuffle(other_accent_files)\n",
    "        \n",
    "print(f\"Number of english accent files: {len(english_accent_files)}\")\n",
    "print(f\"Number of other accent files: {len(other_accent_files)}\")\n",
    "\n",
    "idxs = np.random.randint(100, size=100)\n",
    "oversampled_other = other_accent_files[:100]\n",
    "\n",
    "for n in idxs:\n",
    "    oversampled_other.append(other_accent_files[n])\n",
    "\n",
    "# train_files = english_accent_files[0:end_idx_train] + other_accent_files[0:end_idx_train]\n",
    "# val_files   = english_accent_files[end_idx_train:end_idx_val] + other_accent_files[end_idx_train:end_idx_val]\n",
    "# test_files  = english_accent_files[end_idx_val:end_idx_test] + other_accent_files[end_idx_val:end_idx_test]\n",
    "\n",
    "train_files = english_accent_files[0:end_idx_train] + oversampled_other\n",
    "val_files   = english_accent_files[200:250] + other_accent_files[100:150]\n",
    "test_files  = english_accent_files[250:298] + other_accent_files[150:]\n",
    "\n",
    "print(f\"Number of training files: {len(train_files)}\")\n",
    "print(f\"Number of validaiton files: {len(val_files)}\")\n",
    "print(f\"Number of test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(df, files, size):\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i]['accent'] == 'us' and count1 < size:\n",
    "            files.append((df.loc[i]['filename'], 0))\n",
    "            count1 += 1\n",
    "        elif df.loc[i]['accent'] == 'indian' and count2 < size:\n",
    "            files.append((df.loc[i]['filename'], 1))\n",
    "            count2 += 1\n",
    "\n",
    "        if count1 == size and count2 == size:\n",
    "            break\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 8000\n",
      "Number of validaiton files: 160\n",
      "Number of test files: 160\n"
     ]
    }
   ],
   "source": [
    "#Data setup for kaggle common voice dataset\n",
    "\n",
    "data_train_src =  Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-train')\n",
    "data_val_src=  Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-dev')\n",
    "data_test_src =  Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-test')\n",
    "\n",
    "train_csv = Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-train.csv')\n",
    "val_csv = Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-dev.csv')\n",
    "test_csv = Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-test.csv')\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "val_df = pd.read_csv(val_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "generate_dataset(train_df, train_files, 4000)\n",
    "generate_dataset(val_df, val_files, 80)\n",
    "generate_dataset(test_df, test_files, 80)\n",
    "        \n",
    "\n",
    "print(f\"Number of training files: {len(train_files)}\")\n",
    "print(f\"Number of validaiton files: {len(val_files)}\")\n",
    "print(f\"Number of test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def generate_mfcc_data(mfcc):\n",
    "        mfcc_standardized = np.zeros(mfcc.shape)\n",
    "        for b in range(mfcc.shape[0]):\n",
    "            mfcc_slice = mfcc[b,:]\n",
    "            centered = mfcc_slice - np.mean(mfcc_slice)\n",
    "            if np.std(centered) != 0:\n",
    "                centered_scaled = centered / np.std(centered)\n",
    "\n",
    "            mfcc_standardized[b,:] = centered_scaled\n",
    "\n",
    "        delta1 = librosa.feature.delta(mfcc_standardized, order=1)\n",
    "        delta2 = librosa.feature.delta(mfcc_standardized, order=2)\n",
    "        mfcc_data = np.stack((mfcc_standardized,delta1,delta2))\n",
    "        \n",
    "        return mfcc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path('C:/Users/omar_/Documents/cockatoos/data/assess')\n",
    "path = Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-train')\n",
    "for f in [\"cv-valid-train/sample-000023.mp3\"]:\n",
    "    segs = segment_and_standardize_audio(path/ f,100)\n",
    "    #os.mkdir(Path('C:/Users/omar_/Documents/cockatoos') / \"testingg\")\n",
    "    for i in range(len(segs)):      \n",
    "        print(f\"{i}: {np.count_nonzero(segs[i].get_array_of_samples())}\")\n",
    "    for i, s in enumerate(segs):\n",
    "        if i < 10:\n",
    "            prefix = \"00\"\n",
    "        else:\n",
    "            prefix = \"0\"\n",
    "        s.export(Path('C:/Users/omar_/Documents/cockatoos') / \"testingg\" / f\"chunk{prefix}{i}.wav\" , format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "sound = AudioSegment.from_mp3(path / \"cv-valid-train/sample-000005.mp3\")\n",
    "sound.export(Path('C:/Users/omar_/Documents/cockatoos') / \"testingg\" / \"test.wav\", format=\"wav\")\n",
    "rate, data = wav.read(Path('C:/Users/omar_/Documents/cockatoos') / \"testingg\" / \"test.wav\" )\n",
    "%matplotlib inline\n",
    "plt.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def segment_and_standardize_audio(path, seg_thresh):\n",
    "#     sound_file = AudioSegment.from_mp3(path)\n",
    "#     audio_chunks = split_on_silence(sound_file, \n",
    "#         # must be silent for at least half a second\n",
    "#         min_silence_len = 5,\n",
    "\n",
    "#         # consider it silent if quieter than -16 dBFS\n",
    "#         silence_thresh=0\n",
    "#     )\n",
    "#     standardized_chunks = []\n",
    "    \n",
    "#     for seg in audio_chunks:\n",
    "            \n",
    "#         seg_len = len(seg)\n",
    "\n",
    "#         if seg_len >= seg_thresh:\n",
    "#             seg_standardized = seg[0:seg_thresh]\n",
    "#         else:\n",
    "#             seg_standardized = seg + AudioSegment.silent(duration=(seg_thresh - seg_len))\n",
    "        \n",
    "#         standardized_chunks.append(seg_standardized)\n",
    "                \n",
    "    \n",
    "#     return standardized_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_standardize_audio(path, seg_size):\n",
    "    sound_file = AudioSegment.from_mp3(path)\n",
    "    limit = len(sound_file) // seg_size if len(sound_file) % seg_size == 0 else len(sound_file) // seg_size + 1\n",
    "    chunks = []\n",
    "    for i in range(0,limit):\n",
    "        chunk = sound_file[i * seg_size : (i + 1) * seg_size]\n",
    "        if len(chunk) < seg_size:\n",
    "            chunk = chunk + AudioSegment.silent(duration=(seg_size - len(chunk)))\n",
    "            \n",
    "        if np.count_nonzero(chunk.get_array_of_samples()) > 4500:\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_data(src, dst, files, train, mean=0, std=1):\n",
    "    \n",
    "    counter = 0\n",
    "    seg_thresh = 500\n",
    "    batch_num = 1\n",
    "    mfccs = []\n",
    "    items = []\n",
    "    labels = []\n",
    "    n_mfcc = 20\n",
    "    \n",
    "    for f in files:\n",
    "        \n",
    "        # use for speech accent archive data\n",
    "            \n",
    "#         if \"english\" in f:\n",
    "#             label = 1\n",
    "#         else:\n",
    "#             label = 0\n",
    "\n",
    "\n",
    "#         audio_chunks = segment_and_standardize_audio(src / f, seg_thresh)\n",
    "\n",
    "     \n",
    "        # use for common voice data\n",
    "        label = f[1]\n",
    "        audio_chunks = segment_and_standardize_audio(src / f[0], seg_thresh)\n",
    "        for seg in audio_chunks:\n",
    "                 \n",
    "            samples = seg.get_array_of_samples()\n",
    "            arr = np.array(samples).astype(np.float32)/32768 # 16 bit \n",
    "            arr = librosa.core.resample(arr, seg.frame_rate, 22050, res_type='kaiser_best') \n",
    "                \n",
    "            mfcc = librosa.feature.mfcc(y=arr, sr=22050, n_mfcc=n_mfcc)\n",
    "            #data = generate_mfcc_data(mfcc)\n",
    "            #items.append(data)\n",
    "            mfccs.append(mfcc)\n",
    "            labels.append(label)\n",
    "            \n",
    "#             if train:\n",
    "#                 noise = np.random.normal(0,1, mfcc.shape)\n",
    "#                 mfcc_noisy = mfcc + noise\n",
    "#                 #noisy_data = generate_mfcc_data(mfcc_noisy)\n",
    "#                 #items.append(noisy_data)\n",
    "#                 mfccs.append(mfcc_noisy)\n",
    "#                 labels.append(label)\n",
    "    \n",
    "    all_data = np.vstack(mfccs).reshape(-1,n_mfcc,22)\n",
    "    if train:\n",
    "        mean = all_data.mean(axis=0)\n",
    "        std = all_data.std(axis=0)\n",
    "        all_data = (all_data - mean) / std\n",
    "    else:\n",
    "        all_data = (all_data - mean) / std\n",
    "    \n",
    "    for j in range(all_data.shape[0]):\n",
    "        d = generate_mfcc_data(all_data[j])\n",
    "        items.append(d)\n",
    "        \n",
    "    max_batch_size = len(labels) // 5\n",
    "    for j in range(0,len(items),max_batch_size):\n",
    "        curr_data = items[j:j + max_batch_size]\n",
    "        curr_labels = labels[j:j + max_batch_size]\n",
    "        batch_mfcc = np.vstack(curr_data).reshape(-1,3,n_mfcc,22)\n",
    "        entry = dict()\n",
    "        entry['data'] = batch_mfcc\n",
    "        entry['labels'] = curr_labels\n",
    "        with open(dst / f'data_batch_{batch_num}.pickle', 'wb') as handle:\n",
    "            pickle.dump(entry, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        batch_num += 1\n",
    "        \n",
    "    if train:\n",
    "        return mean, std\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training words created\n",
      "Validaiton words created\n",
      "Testing words created\n"
     ]
    }
   ],
   "source": [
    "#Uncomment to create the files for the dataset folders (common voice)\n",
    "\n",
    "mean, std = generate_model_data(data_train_src, train_dir, train_files, True)\n",
    "print(\"Training words created\")\n",
    "generate_model_data(data_val_src, val_dir, val_files, False, mean, std)\n",
    "print(\"Validaiton words created\")\n",
    "generate_model_data(data_test_src, test_dir, test_files, False, mean, std)\n",
    "print(\"Testing words created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training words created\n",
      "Validaiton words created\n",
      "Testing words created\n"
     ]
    }
   ],
   "source": [
    "#Uncomment to create the files for the dataset folders (speech accent archive)\n",
    "\n",
    "mean, std = generate_model_data(original_dir, train_dir, train_files, True)\n",
    "print(\"Training words created\")\n",
    "generate_model_data(original_dir, val_dir, val_files, False, mean, std)\n",
    "print(\"Validaiton words created\")\n",
    "generate_model_data(original_dir, test_dir, test_files, False, mean, std)\n",
    "print(\"Testing words created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = Path('C:/Users/omar_/Documents/cockatoos/data/train')\n",
    "val_data_dir = Path('C:/Users/omar_/Documents/cockatoos/data/val')\n",
    "test_data_dir = Path('C:/Users/omar_/Documents/cockatoos/data/test')\n",
    "\n",
    "train_data = MFCCDataset(train_data_dir)\n",
    "val_data = MFCCDataset(val_data_dir)\n",
    "# test_data = MFCCDataset(test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15448\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30896\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20, 22])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model definition\n",
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten(1,3),\n",
    "            nn.Linear(768,300),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(300,1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Flatten(start_dim=1, end_dim=3)\n",
      "  (10): Linear(in_features=768, out_features=128, bias=True)\n",
      "  (11): Dropout(p=0.5, inplace=False)\n",
      "  (12): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (13): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,batch_size=128,shuffle=True)\n",
    "val_loader = DataLoader(val_data,batch_size=32,shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "epochs = 100\n",
    "max_val_acc = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = nn.BCELoss()(outputs,labels.to(device).reshape(-1,1))\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputs = outputs.reshape(1, -1)\n",
    "        outputs = outputs.squeeze()\n",
    "        for i in range(outputs.size()[0]):\n",
    "            if (labels[i] == 0 and outputs[i] < 0.5) or (labels[i] == 1 and outputs[i] >= 0.5):\n",
    "                correct += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            for j, (d,l) in enumerate(val_loader):\n",
    "                o = model(d.to(device))\n",
    "                loss = nn.BCELoss()(o,l.to(device).reshape(-1,1))\n",
    "                val_loss += loss.item()\n",
    "                o = o.reshape(1,-1)\n",
    "                o = o.squeeze()\n",
    "                for i in range(o.size()[0]):\n",
    "                    if (l[i] == 0 and o[i] < 0.5) or (l[i] == 1 and o[i] >= 0.5):\n",
    "                        val_correct += 1\n",
    "\n",
    "            accuracy = 100 * val_correct / len(val_data)\n",
    "            if accuracy > max_val_acc:\n",
    "                print('new record making model!')\n",
    "                max_val_acc = accuracy\n",
    "                #torch.save(model.state_dict(), args.model_path+'/model.pt')\n",
    "            print(f\"Validation loss for epoch {epoch}: {val_loss / len(val_loader)}, accuracy: {accuracy}\")\n",
    "\n",
    "    accuracy = 100 * correct / len(train_data)\n",
    "    print(f\"Epoch:{epoch}, avg loss for epoch:{running_loss / len(train_loader)}, accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifying specific set of audio samples\n",
    "def predict(test_dir):\n",
    "    \n",
    "    predictions = dict()\n",
    "    for f in os.listdir(test_dir):\n",
    "        audio_chunks = segment_and_standardize_audio(test_dir / f, 500)\n",
    "        num_english_pred = 0\n",
    "        for seg in audio_chunks:\n",
    "\n",
    "            samples = seg.get_array_of_samples()\n",
    "            arr = np.array(samples).astype(np.float32)/32768 # 16 bit \n",
    "            arr = librosa.core.resample(arr, seg.frame_rate, 22050, res_type='kaiser_best') \n",
    "\n",
    "            mfcc = librosa.feature.mfcc(y=arr, sr=22050)\n",
    "            data = generate_mfcc_data(mfcc)\n",
    "            pred = model(torch.from_numpy(data).unsqueeze(0).float().to(device)).item()\n",
    "            if pred > 0.5:\n",
    "                num_english_pred += 1\n",
    "        \n",
    "        frac_english_preds = num_english_pred / len(audio_chunks)\n",
    "        \n",
    "        if frac_english_preds >= 0.5:\n",
    "            predictions[f] = 1\n",
    "        else:\n",
    "            predictions[f] = 0\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4076\n",
      "637\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "speaker_csv = Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-dev.csv')\n",
    "df = pd.read_csv(speaker_csv)\n",
    "print(len(df))\n",
    "count = 0\n",
    "#print(df.loc[0])\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "d = dict()\n",
    "count = 0\n",
    "for i in range(len(df)):\n",
    "    if df.loc[i]['accent'] == \"us\":\n",
    "        count1 += 1\n",
    "    elif df.loc[i]['accent'] == \"indian\":\n",
    "        count2 += 1\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "#     if df.loc[i]['accent'] == 'us':\n",
    "#         count1 += 1\n",
    "#     elif df.loc[i]['accent'] == 'indian':\n",
    "#         count2 += 1\n",
    "        \n",
    "print(count1)\n",
    "print(count2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

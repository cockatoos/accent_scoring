{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from pydub import AudioSegment, silence\n",
    "from pydub.silence import split_on_silence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "              \n",
    "        self.mfccs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for i in range(len(os.listdir(self.root_dir))):\n",
    "\n",
    "            f = os.listdir(self.root_dir)[i]\n",
    "            with open(self.root_dir / f, 'rb') as handle:\n",
    "                entry = pickle.load(handle)\n",
    "            self.mfccs.append(entry['data'])\n",
    "            self.labels.extend(entry['labels'])\n",
    "        \n",
    "        self.mfccs = torch.from_numpy(np.vstack(self.mfccs).reshape(-1,3,50,44)).float()\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.mfccs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.mfccs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(df, files, accents, sizes):\n",
    "    for accent in accents:\n",
    "        if accent == 'us':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        accent_df = df[df['accent'] == accent]\n",
    "        filenames = accent_df['filename'].tolist()\n",
    "        if accent in sizes:\n",
    "            filenames = filenames[:sizes[accent]]\n",
    "        for name in filenames:\n",
    "            files.append((name,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 12166\n",
      "Number of test files: 287\n"
     ]
    }
   ],
   "source": [
    "#Data setup for kaggle common voice dataset\n",
    "\n",
    "train_dir =  Path('C:/Users/omar_/Documents/cockatoos/data/train')\n",
    "test_dir =  Path('C:/Users/omar_/Documents/cockatoos/data/test')\n",
    "\n",
    "data_train_src =  Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-train')\n",
    "data_test_src =  Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-test')\n",
    "\n",
    "train_csv = Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-train.csv')\n",
    "test_csv = Path('C:/Users/omar_/Documents/kaggle_voice/archive (1)/cv-valid-test.csv')\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "\n",
    "accents = ['malaysia', 'african', 'wales', 'philippines','hongkong','singapore', 'indian', 'us']\n",
    "train_sizes = {'us': 6000, 'indian': 4000}\n",
    "generate_dataset(train_df, train_files, accents, train_sizes)\n",
    "test_sizes = {'us': 150}\n",
    "generate_dataset(test_df, test_files, accents, test_sizes)\n",
    "\n",
    "        \n",
    "print(f\"Number of training files: {len(train_files)}\")\n",
    "print(f\"Number of test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def generate_mfcc_data(mfcc):\n",
    "        mfcc_standardized = np.zeros(mfcc.shape)\n",
    "        for b in range(mfcc.shape[0]):\n",
    "            mfcc_slice = mfcc[b,:]\n",
    "            centered = mfcc_slice - np.mean(mfcc_slice)\n",
    "            if np.std(centered) != 0:\n",
    "                centered_scaled = centered / np.std(centered)\n",
    "\n",
    "            mfcc_standardized[b,:] = centered_scaled\n",
    "\n",
    "        delta1 = librosa.feature.delta(mfcc_standardized, order=1)\n",
    "        delta2 = librosa.feature.delta(mfcc_standardized, order=2)\n",
    "        mfcc_data = np.stack((mfcc_standardized,delta1,delta2))\n",
    "        \n",
    "        return mfcc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_and_standardize_audio(path, seg_size):\n",
    "    sound_file = AudioSegment.from_mp3(path)\n",
    "    limit = len(sound_file) // seg_size if len(sound_file) % seg_size == 0 else len(sound_file) // seg_size + 1\n",
    "    chunks = []\n",
    "    for i in range(0,limit):\n",
    "        chunk = sound_file[i * seg_size : (i + 1) * seg_size]\n",
    "        if len(chunk) < seg_size:\n",
    "            chunk = chunk + AudioSegment.silent(duration=(seg_size - len(chunk)))\n",
    "          \n",
    "\n",
    "        if np.count_nonzero(chunk.get_array_of_samples()) > 45000:\n",
    "            chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_data(src, dst, files, train, mean=0, std=1):\n",
    "    \n",
    "    counter = 0\n",
    "    seg_size = 1000\n",
    "    batch_num = 1\n",
    "    mfccs = []\n",
    "    items = []\n",
    "    labels = []\n",
    "    n_mfcc = 50\n",
    "    mfcc_width = 44\n",
    "    c = 0\n",
    "    \n",
    "    for f in files:\n",
    "\n",
    "     \n",
    "        # use for common voice data\n",
    "        label = f[1]\n",
    "        audio_chunks = segment_and_standardize_audio(src / f[0], seg_size)\n",
    "        for seg in audio_chunks:\n",
    "                 \n",
    "            samples = seg.get_array_of_samples()\n",
    "            arr = np.array(samples).astype(np.float32)/32768 # 16 bit \n",
    "            arr = librosa.core.resample(arr, seg.frame_rate, 22050, res_type='kaiser_best') \n",
    "                \n",
    "            mfcc = librosa.feature.mfcc(y=arr, sr=22050, n_mfcc=n_mfcc)\n",
    "            mfccs.append(mfcc)\n",
    "            labels.append(label)\n",
    "            \n",
    "        c += 1\n",
    "        if c % 100 == 0:\n",
    "            print(f\"Processed {c} files\")\n",
    "        \n",
    "        \n",
    "    all_data = np.vstack(mfccs).reshape(-1,n_mfcc,mfcc_width)\n",
    "    if train:\n",
    "        mean = all_data.mean(axis=0)\n",
    "        std = all_data.std(axis=0)\n",
    "        all_data = (all_data - mean) / std\n",
    "    else:\n",
    "        all_data = (all_data - mean) / std\n",
    "    \n",
    "    for j in range(all_data.shape[0]):\n",
    "        d = generate_mfcc_data(all_data[j])\n",
    "        items.append(d)\n",
    "    \n",
    "    \n",
    "    batch_size = len(labels) // 6\n",
    "    for j in range(6):\n",
    "        \n",
    "        start = j * batch_size\n",
    "        end = start + batch_size\n",
    "        if j == 5 and len(labels) % 6 != 0:\n",
    "            end = len(labels)\n",
    "        curr_data = items[start:end]\n",
    "        curr_labels = labels[start:end]\n",
    "        batch_mfcc = np.vstack(curr_data).reshape(-1,3,n_mfcc,mfcc_width)\n",
    "        entry = dict()\n",
    "        entry['data'] = batch_mfcc\n",
    "        entry['labels'] = curr_labels\n",
    "        with open(dst / f'data_batch_{j+1}.pickle', 'wb') as handle:\n",
    "            pickle.dump(entry, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "        \n",
    "    if train:\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 files\n",
      "Processed 200 files\n",
      "Testing words created\n"
     ]
    }
   ],
   "source": [
    "#Uncomment to create the files for the dataset folders (common voice)\n",
    "\n",
    "# mean, std = generate_model_data(data_train_src, train_dir, train_files, True)\n",
    "# print(\"Training words created\")\n",
    "# generate_model_data(data_test_src, test_dir, test_files, False, mean, std)\n",
    "# print(\"Testing words created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = Path('C:/Users/omar_/Documents/cockatoos/data/train')\n",
    "test_data_dir = Path('C:/Users/omar_/Documents/cockatoos/data/test')\n",
    "\n",
    "train_data = MFCCDataset(train_data_dir)\n",
    "test_data = MFCCDataset(test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccentClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3,32,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Flatten(1,3),\n",
    "            nn.Linear(6336,256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  Loss: 0.9060640122854349  Accuracy: 57.743598862019915\n",
      "Epoch 2  Loss: 0.6456442260832498  Accuracy: 61.79468942626838\n",
      "Epoch 3  Loss: 0.6373144142103918  Accuracy: 63.20531057373162\n",
      "Epoch 4  Loss: 0.6283428409334385  Accuracy: 63.69724988146041\n",
      "Epoch 5  Loss: 0.6233479913437006  Accuracy: 64.66927453769559\n",
      "Epoch 6  Loss: 0.6146097856037545  Accuracy: 65.28568041725937\n",
      "Epoch 7  Loss: 0.6082637279548428  Accuracy: 66.29919393077287\n",
      "Epoch 8  Loss: 0.6000686247240413  Accuracy: 67.14378852536747\n",
      "Epoch 9  Loss: 0.5913282931979859  Accuracy: 67.83428165007112\n",
      "Epoch 10  Loss: 0.5878005685905615  Accuracy: 68.27584163110478\n",
      "Epoch 11  Loss: 0.5818150925139586  Accuracy: 68.70554765291607\n",
      "Epoch 12  Loss: 0.5738031159747731  Accuracy: 69.46123755334281\n",
      "Epoch 13  Loss: 0.5711130625145002  Accuracy: 69.5945945945946\n",
      "Epoch 14  Loss: 0.5616213870093678  Accuracy: 70.41251778093883\n",
      "Epoch 15  Loss: 0.5580187040296468  Accuracy: 70.87482219061167\n",
      "Epoch 16  Loss: 0.5475855957378041  Accuracy: 71.77275960170697\n",
      "Epoch 17  Loss: 0.5441219291903756  Accuracy: 71.73423423423424\n",
      "Epoch 18  Loss: 0.539384554055604  Accuracy: 72.43658131816026\n",
      "Epoch 19  Loss: 0.5338191300856344  Accuracy: 72.69440493124704\n",
      "Epoch 20  Loss: 0.531277107018413  Accuracy: 72.8396159317212\n",
      "Epoch 21  Loss: 0.5219304403572371  Accuracy: 73.39378852536747\n",
      "Epoch 22  Loss: 0.519957816623377  Accuracy: 73.85016595542912\n",
      "Epoch 23  Loss: 0.5181172826524937  Accuracy: 73.84127548601232\n",
      "Epoch 24  Loss: 0.510614684585369  Accuracy: 74.30357989568516\n",
      "Epoch 25  Loss: 0.5057447477046287  Accuracy: 74.24727358937885\n",
      "Epoch 26  Loss: 0.5056577968326482  Accuracy: 74.89924134660977\n",
      "Epoch 27  Loss: 0.49806442710034776  Accuracy: 75.34969179706022\n",
      "Epoch 28  Loss: 0.4920078845638217  Accuracy: 75.47712185870081\n",
      "Epoch 29  Loss: 0.4871573146771301  Accuracy: 75.97498814604077\n",
      "Epoch 30  Loss: 0.4875719950280406  Accuracy: 75.92757230915126\n",
      "Epoch 31  Loss: 0.48529029592420114  Accuracy: 76.31282598387861\n",
      "Epoch 32  Loss: 0.4813225373174205  Accuracy: 76.22984826932195\n",
      "Epoch 33  Loss: 0.4765367198622588  Accuracy: 77.0477714556662\n",
      "Epoch 34  Loss: 0.47420275143601676  Accuracy: 76.90552394499763\n",
      "Epoch 35  Loss: 0.4707231882846717  Accuracy: 77.14260312944523\n",
      "Epoch 36  Loss: 0.466186814913244  Accuracy: 77.10111427216691\n",
      "Epoch 37  Loss: 0.4663125977597453  Accuracy: 77.42709815078236\n",
      "Epoch 38  Loss: 0.46099547951510456  Accuracy: 77.88051209103841\n",
      "Epoch 39  Loss: 0.46264292920629185  Accuracy: 77.87458511142722\n",
      "Epoch 40  Loss: 0.4545546118734461  Accuracy: 78.39023233760076\n",
      "Epoch 41  Loss: 0.4552901459462715  Accuracy: 78.14129919393078\n",
      "Epoch 42  Loss: 0.45108188688755035  Accuracy: 78.45246562351825\n",
      "Epoch 43  Loss: 0.4517733154423309  Accuracy: 78.69547178757705\n",
      "Epoch 44  Loss: 0.4492839708021193  Accuracy: 78.69547178757705\n",
      "Epoch 45  Loss: 0.44362168115648354  Accuracy: 78.81697486960645\n",
      "Epoch 46  Loss: 0.44320974679607333  Accuracy: 78.98885727833097\n",
      "Epoch 47  Loss: 0.4420494358196403  Accuracy: 79.2674253200569\n",
      "Epoch 48  Loss: 0.4389570445725412  Accuracy: 79.14888572783309\n",
      "Epoch 49  Loss: 0.43942186924995796  Accuracy: 79.24075391180655\n",
      "Epoch 50  Loss: 0.43655372083638655  Accuracy: 79.46301564722617\n",
      "Epoch 51  Loss: 0.4381598070489638  Accuracy: 79.18148411569464\n",
      "Epoch 52  Loss: 0.43718207875887555  Accuracy: 79.16073968705548\n",
      "Epoch 53  Loss: 0.4373956188333757  Accuracy: 79.35336652441916\n",
      "Epoch 54  Loss: 0.42535932474967203  Accuracy: 79.99644381223328\n",
      "Epoch 55  Loss: 0.42415578629482875  Accuracy: 79.91939307728781\n",
      "Epoch 56  Loss: 0.42938351744052133  Accuracy: 79.66453295400663\n",
      "Epoch 57  Loss: 0.4253084599745996  Accuracy: 80.26315789473684\n",
      "Epoch 58  Loss: 0.42405560782009905  Accuracy: 80.19499762920816\n",
      "Epoch 59  Loss: 0.42241781103340065  Accuracy: 80.19203413940257\n",
      "Epoch 60  Loss: 0.422784677396218  Accuracy: 80.03793266951162\n",
      "Epoch 61  Loss: 0.41810242267269077  Accuracy: 80.67508297771455\n",
      "Epoch 62  Loss: 0.41425046516638814  Accuracy: 80.66619250829777\n",
      "Epoch 63  Loss: 0.418857054502675  Accuracy: 80.31057373162636\n",
      "Epoch 64  Loss: 0.4114758320378535  Accuracy: 80.70768136557611\n",
      "Epoch 65  Loss: 0.418629201637073  Accuracy: 80.38169748696065\n",
      "Epoch 66  Loss: 0.4114996882324869  Accuracy: 80.7699146514936\n",
      "Epoch 67  Loss: 0.41160419233369105  Accuracy: 80.80844001896634\n",
      "Epoch 68  Loss: 0.41240484594847215  Accuracy: 81.06626363205311\n",
      "Epoch 69  Loss: 0.40931156650185585  Accuracy: 80.92697961119013\n",
      "Epoch 70  Loss: 0.41045955500819464  Accuracy: 80.80547652916074\n",
      "Epoch 71  Loss: 0.40697666517261305  Accuracy: 81.24407302038881\n",
      "Epoch 72  Loss: 0.40414402001734934  Accuracy: 81.53449502133712\n",
      "Epoch 73  Loss: 0.408343599714113  Accuracy: 81.10182550972024\n",
      "Epoch 74  Loss: 0.40439876247987605  Accuracy: 81.07515410146989\n",
      "Epoch 75  Loss: 0.4016729501838034  Accuracy: 81.63821716453296\n",
      "Epoch 76  Loss: 0.40611431115504465  Accuracy: 81.08404457088668\n",
      "Epoch 77  Loss: 0.40331183678724547  Accuracy: 81.30037932669512\n",
      "Epoch 78  Loss: 0.3988386898329764  Accuracy: 81.58487434803224\n",
      "Epoch 79  Loss: 0.39964558601830946  Accuracy: 81.60265528686581\n",
      "Epoch 80  Loss: 0.39918720112605527  Accuracy: 81.69155998103366\n",
      "Epoch 81  Loss: 0.39483149315823207  Accuracy: 81.82491702228545\n",
      "Epoch 82  Loss: 0.3986873941665346  Accuracy: 81.38928402086297\n",
      "Epoch 83  Loss: 0.396744644325791  Accuracy: 81.71823138928401\n",
      "Epoch 84  Loss: 0.39255682333852304  Accuracy: 81.93752963489806\n",
      "Epoch 85  Loss: 0.3921123903922059  Accuracy: 81.72415836889522\n",
      "Epoch 86  Loss: 0.394322866517486  Accuracy: 82.02347083926031\n",
      "Epoch 87  Loss: 0.3957578239567352  Accuracy: 81.93752963489806\n",
      "Epoch 88  Loss: 0.3902702661174716  Accuracy: 82.18646277856804\n",
      "Epoch 89  Loss: 0.3898865632487066  Accuracy: 82.2072072072072\n",
      "Epoch 90  Loss: 0.3887434395199472  Accuracy: 82.10941204362257\n",
      "Epoch 91  Loss: 0.3858944751096494  Accuracy: 82.3346372688478\n",
      "Epoch 92  Loss: 0.39026130826184247  Accuracy: 82.30203888098625\n",
      "Epoch 93  Loss: 0.3858958620465163  Accuracy: 82.28425794215268\n",
      "Epoch 94  Loss: 0.38320985656570306  Accuracy: 82.64284020862968\n",
      "Epoch 95  Loss: 0.3865858933916598  Accuracy: 82.46799431009957\n",
      "Epoch 96  Loss: 0.378985616406708  Accuracy: 82.9362256993836\n",
      "Epoch 97  Loss: 0.3821504356508905  Accuracy: 82.65469416785206\n",
      "Epoch 98  Loss: 0.3833177668804472  Accuracy: 82.5776434329066\n",
      "Epoch 99  Loss: 0.3832998596357577  Accuracy: 82.52430061640588\n",
      "Epoch 100  Loss: 0.382035149092024  Accuracy: 82.74359886201991\n",
      "Model test accuracy for fold 0: 65.81333333333333 \n",
      "Epoch 1  Loss: 52.9518727524714  Accuracy: 46.692745376955905\n",
      "Epoch 2  Loss: 53.34398674242424  Accuracy: 46.651256519677574\n",
      "Epoch 3  Loss: 53.35996685606061  Accuracy: 46.651256519677574\n",
      "Epoch 4  Loss: 53.352864583333336  Accuracy: 46.651256519677574\n",
      "Epoch 5  Loss: 53.34398674242424  Accuracy: 46.651256519677574\n",
      "Epoch 6  Loss: 53.35464015151515  Accuracy: 46.651256519677574\n",
      "Epoch 7  Loss: 53.35819128787879  Accuracy: 46.651256519677574\n",
      "Epoch 8  Loss: 53.36174242424242  Accuracy: 46.651256519677574\n",
      "Epoch 9  Loss: 53.35464015151515  Accuracy: 46.651256519677574\n",
      "Epoch 10  Loss: 53.34043560606061  Accuracy: 46.651256519677574\n",
      "Epoch 11  Loss: 33.55546067497044  Accuracy: 49.62660028449502\n",
      "Epoch 12  Loss: 0.6730771933992704  Accuracy: 58.78674727358938\n",
      "Epoch 13  Loss: 0.6533176185506763  Accuracy: 60.80488383119962\n",
      "Epoch 14  Loss: 0.6432895847793781  Accuracy: 61.85099573257468\n",
      "Epoch 15  Loss: 0.6341468289946065  Accuracy: 63.12233285917497\n",
      "Epoch 16  Loss: 0.6285420394305027  Accuracy: 63.477951635846374\n",
      "Epoch 17  Loss: 0.6207170527089726  Accuracy: 64.28402086296823\n",
      "Epoch 18  Loss: 0.6149029503717567  Accuracy: 65.01007586533902\n",
      "Epoch 19  Loss: 0.6063952001206803  Accuracy: 66.20732574679943\n",
      "Epoch 20  Loss: 0.5995829170851996  Accuracy: 66.7703888098625\n",
      "Epoch 21  Loss: 0.5927315025844357  Accuracy: 67.37494073020389\n",
      "Epoch 22  Loss: 0.5852817669510841  Accuracy: 68.10395922238027\n",
      "Epoch 23  Loss: 0.5800285208405871  Accuracy: 68.60775248933143\n",
      "Epoch 24  Loss: 0.5733815577219833  Accuracy: 69.27157420578473\n",
      "Epoch 25  Loss: 0.567899330434474  Accuracy: 69.48790896159318\n",
      "Epoch 26  Loss: 0.5623537375394142  Accuracy: 69.75165955429114\n",
      "Epoch 27  Loss: 0.5553711487939863  Accuracy: 70.80369843527738\n",
      "Epoch 28  Loss: 0.5548633181236007  Accuracy: 70.75924608819345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29  Loss: 0.5477148285417845  Accuracy: 71.60976766239924\n",
      "Epoch 30  Loss: 0.5448932427574288  Accuracy: 71.79054054054055\n",
      "Epoch 31  Loss: 0.5389589455091592  Accuracy: 72.26173541963016\n",
      "Epoch 32  Loss: 0.5288723971356045  Accuracy: 73.05891417733523\n",
      "Epoch 33  Loss: 0.5278610706780896  Accuracy: 73.2604314841157\n",
      "Epoch 34  Loss: 0.5268077480070519  Accuracy: 73.21005215742058\n",
      "Epoch 35  Loss: 0.5195595589325284  Accuracy: 73.78793266951162\n",
      "Epoch 36  Loss: 0.5143134347868689  Accuracy: 74.48731626363205\n",
      "Epoch 37  Loss: 0.5116160744518945  Accuracy: 74.39248458985301\n",
      "Epoch 38  Loss: 0.5105879961541204  Accuracy: 74.6443812233286\n",
      "Epoch 39  Loss: 0.5061274710478205  Accuracy: 74.58807491702228\n",
      "Epoch 40  Loss: 0.5029490382382364  Accuracy: 75.12150308202939\n",
      "Epoch 41  Loss: 0.4955072837571303  Accuracy: 75.51564722617354\n",
      "Epoch 42  Loss: 0.49445676713278797  Accuracy: 75.68752963489806\n",
      "Epoch 43  Loss: 0.49331162813486473  Accuracy: 75.94535324798483\n",
      "Epoch 44  Loss: 0.4871337322348898  Accuracy: 76.01055002370792\n",
      "Epoch 45  Loss: 0.4868808056820523  Accuracy: 75.99276908487435\n",
      "Epoch 46  Loss: 0.4832681429882844  Accuracy: 76.58843053579896\n",
      "Epoch 47  Loss: 0.4781286070744197  Accuracy: 76.81958274063537\n",
      "Epoch 48  Loss: 0.47560840811241756  Accuracy: 76.74845898530108\n",
      "Epoch 49  Loss: 0.4723363589834083  Accuracy: 76.98850165955429\n",
      "Epoch 50  Loss: 0.47272781892256305  Accuracy: 77.18409198672357\n",
      "Epoch 51  Loss: 0.4692189632491632  Accuracy: 77.43302513039355\n",
      "Epoch 52  Loss: 0.46571835311073245  Accuracy: 77.35004741583688\n",
      "Epoch 53  Loss: 0.46538412887038605  Accuracy: 77.39746325272641\n",
      "Epoch 54  Loss: 0.46131475789077353  Accuracy: 77.75604551920341\n",
      "Epoch 55  Loss: 0.46680503711104393  Accuracy: 77.68492176386913\n",
      "Epoch 56  Loss: 0.45667171387961414  Accuracy: 78.55026078710289\n",
      "Epoch 57  Loss: 0.4550052017650821  Accuracy: 78.09684684684684\n",
      "Epoch 58  Loss: 0.45726299771305284  Accuracy: 78.08499288762447\n",
      "Epoch 59  Loss: 0.4534028915293289  Accuracy: 78.1264817449028\n",
      "Epoch 60  Loss: 0.4494472525336526  Accuracy: 78.76363205310574\n",
      "Epoch 61  Loss: 0.4498200961811976  Accuracy: 78.55915125651968\n",
      "Epoch 62  Loss: 0.4529586519030007  Accuracy: 78.29243717401611\n",
      "Epoch 63  Loss: 0.44912960956042464  Accuracy: 78.4880275011854\n",
      "Epoch 64  Loss: 0.4471495161679658  Accuracy: 78.89402560455191\n",
      "Epoch 65  Loss: 0.4458312802016735  Accuracy: 78.93551446183025\n",
      "Epoch 66  Loss: 0.4397582435472445  Accuracy: 79.46301564722617\n",
      "Epoch 67  Loss: 0.43948044594038616  Accuracy: 78.97700331910858\n",
      "Epoch 68  Loss: 0.44192180105231027  Accuracy: 79.05405405405405\n",
      "Epoch 69  Loss: 0.4351456322227464  Accuracy: 79.54302987197724\n",
      "Epoch 70  Loss: 0.4322966303337704  Accuracy: 79.71194879089616\n",
      "Epoch 71  Loss: 0.4343686668258725  Accuracy: 79.43930772878142\n",
      "Epoch 72  Loss: 0.4307962732784676  Accuracy: 79.8067804646752\n",
      "Epoch 73  Loss: 0.43441255131002626  Accuracy: 79.48079658605974\n",
      "Epoch 74  Loss: 0.4315448711541566  Accuracy: 79.62008060692271\n",
      "Epoch 75  Loss: 0.4291389952554847  Accuracy: 79.81567093409198\n",
      "Epoch 76  Loss: 0.4312208012649507  Accuracy: 79.99051683262209\n",
      "Epoch 77  Loss: 0.42843102799220517  Accuracy: 79.94902797534377\n",
      "Epoch 78  Loss: 0.4213480019208157  Accuracy: 80.47652916073969\n",
      "Epoch 79  Loss: 0.42728019088055147  Accuracy: 80.12683736367947\n",
      "Epoch 80  Loss: 0.4239748028868979  Accuracy: 80.3520625889047\n",
      "Epoch 81  Loss: 0.41995133538589335  Accuracy: 80.55357989568516\n",
      "Epoch 82  Loss: 0.42071786534154054  Accuracy: 80.53876244665719\n",
      "Epoch 83  Loss: 0.41827911364309717  Accuracy: 80.37873399715505\n",
      "Epoch 84  Loss: 0.42002301279342535  Accuracy: 80.52394499762921\n",
      "Epoch 85  Loss: 0.4168102965887749  Accuracy: 80.75509720246562\n",
      "Epoch 86  Loss: 0.4159704295523239  Accuracy: 80.78769559032717\n",
      "Epoch 87  Loss: 0.4163574185786825  Accuracy: 80.55357989568516\n",
      "Epoch 88  Loss: 0.41640391087893286  Accuracy: 80.43207681365575\n",
      "Epoch 89  Loss: 0.4146395619168426  Accuracy: 80.83807491702228\n",
      "Epoch 90  Loss: 0.41360715088067634  Accuracy: 80.6513750592698\n",
      "Epoch 91  Loss: 0.4107483653633883  Accuracy: 81.00403034613561\n",
      "Epoch 92  Loss: 0.40730678843277873  Accuracy: 81.27963489805595\n",
      "Epoch 93  Loss: 0.41126325602332753  Accuracy: 80.98921289710763\n",
      "Epoch 94  Loss: 0.4093603443241481  Accuracy: 81.04551920341395\n",
      "Epoch 95  Loss: 0.4088099299168045  Accuracy: 81.20851114272168\n",
      "Epoch 96  Loss: 0.4060100804675709  Accuracy: 81.1907302038881\n",
      "Epoch 97  Loss: 0.40639878944917157  Accuracy: 81.20258416311047\n",
      "Epoch 98  Loss: 0.40180952621228766  Accuracy: 81.56412991939308\n",
      "Epoch 99  Loss: 0.4033974035445488  Accuracy: 81.37743006164058\n",
      "Epoch 100  Loss: 0.4024848479664687  Accuracy: 81.5493124703651\n",
      "Model test accuracy for fold 1: 67.09333333333333 \n",
      "Epoch 1  Loss: 52.98813270280758  Accuracy: 46.68385490753912\n",
      "Epoch 2  Loss: 53.38127367424242  Accuracy: 46.60680417259365\n",
      "Epoch 3  Loss: 53.39547821969697  Accuracy: 46.60680417259365\n",
      "Epoch 4  Loss: 53.407907196969695  Accuracy: 46.60680417259365\n",
      "Epoch 5  Loss: 53.391927083333336  Accuracy: 46.60680417259365\n",
      "Epoch 6  Loss: 53.39902935606061  Accuracy: 46.60680417259365\n",
      "Epoch 7  Loss: 53.39725378787879  Accuracy: 46.60680417259365\n",
      "Epoch 8  Loss: 12.670219021764668  Accuracy: 53.20353247984827\n",
      "Epoch 9  Loss: 0.674004509367726  Accuracy: 58.34222380275012\n",
      "Epoch 10  Loss: 0.6606738316741857  Accuracy: 59.82396870554765\n",
      "Epoch 11  Loss: 0.6504550287217805  Accuracy: 60.941204362257\n",
      "Epoch 12  Loss: 0.6418367646860353  Accuracy: 61.76801801801802\n",
      "Epoch 13  Loss: 0.6308771866289052  Accuracy: 63.21123755334282\n",
      "Epoch 14  Loss: 0.6204050501639192  Accuracy: 64.71965386439071\n",
      "Epoch 15  Loss: 0.6132770772233154  Accuracy: 65.49905168326221\n",
      "Epoch 16  Loss: 0.6003717759793455  Accuracy: 66.81484115694641\n",
      "Epoch 17  Loss: 0.594195241052093  Accuracy: 67.19713134186819\n",
      "Epoch 18  Loss: 0.5865858068520372  Accuracy: 68.032835467046\n",
      "Epoch 19  Loss: 0.5738423202525486  Accuracy: 69.17970602181128\n",
      "Epoch 20  Loss: 0.5723219406649922  Accuracy: 69.55014224751066\n",
      "Epoch 21  Loss: 0.5647215853360567  Accuracy: 70.50142247510668\n",
      "Epoch 22  Loss: 0.5594780086116358  Accuracy: 70.53994784257942\n",
      "Epoch 23  Loss: 0.552433035025994  Accuracy: 71.01114272166903\n",
      "Epoch 24  Loss: 0.543241641738198  Accuracy: 71.97131341868185\n",
      "Epoch 25  Loss: 0.5408217113352183  Accuracy: 72.40101944049313\n",
      "Epoch 26  Loss: 0.5340528291734782  Accuracy: 72.72403982930298\n",
      "Epoch 27  Loss: 0.5266091524425781  Accuracy: 73.18930772878142\n",
      "Epoch 28  Loss: 0.528077855141777  Accuracy: 73.2011616880038\n",
      "Epoch 29  Loss: 0.5195162330161441  Accuracy: 73.88572783309625\n",
      "Epoch 30  Loss: 0.5128240874319365  Accuracy: 74.25912754860123\n",
      "Epoch 31  Loss: 0.5073704386525082  Accuracy: 74.95851114272168\n",
      "Epoch 32  Loss: 0.5058045311633385  Accuracy: 74.80737316263632\n",
      "Epoch 33  Loss: 0.5009388187617967  Accuracy: 75.00889046941678\n",
      "Epoch 34  Loss: 0.4976055573559169  Accuracy: 75.40896159317212\n",
      "Epoch 35  Loss: 0.4927494190633297  Accuracy: 75.82977714556662\n",
      "Epoch 36  Loss: 0.4919588416814804  Accuracy: 75.76754385964912\n",
      "Epoch 37  Loss: 0.4874547097944852  Accuracy: 76.24762920815553\n",
      "Epoch 38  Loss: 0.48217456313696777  Accuracy: 76.43136557610242\n",
      "Epoch 39  Loss: 0.4770269136537205  Accuracy: 76.78402086296823\n",
      "Epoch 40  Loss: 0.47997397082773124  Accuracy: 76.47581792318634\n",
      "Epoch 41  Loss: 0.47125443714586174  Accuracy: 77.18409198672357\n",
      "Epoch 42  Loss: 0.4722843991987633  Accuracy: 77.05073494547179\n",
      "Epoch 43  Loss: 0.46901515995462734  Accuracy: 77.56638217164533\n",
      "Epoch 44  Loss: 0.46727241372520273  Accuracy: 77.57527264106211\n",
      "Epoch 45  Loss: 0.4633214549359047  Accuracy: 77.56638217164533\n",
      "Epoch 46  Loss: 0.4594196645599423  Accuracy: 77.75011853959222\n",
      "Epoch 47  Loss: 0.455514109044364  Accuracy: 78.41394025604552\n",
      "Epoch 48  Loss: 0.45586781056992937  Accuracy: 78.34874348032243\n",
      "Epoch 49  Loss: 0.4525223628363826  Accuracy: 78.41394025604552\n",
      "Epoch 50  Loss: 0.4485889182172038  Accuracy: 78.59471313418682\n",
      "Epoch 51  Loss: 0.4468795928088101  Accuracy: 78.9651493598862\n",
      "Epoch 52  Loss: 0.4470620189200748  Accuracy: 78.70436225699383\n",
      "Epoch 53  Loss: 0.44164859638972714  Accuracy: 79.03034613560929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-14d175d7dcbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                     \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_data)):\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "                      train_data, \n",
    "                      batch_size=128, sampler=train_subsampler)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "                      train_data,\n",
    "                      batch_size=32, sampler=test_subsampler)\n",
    "    \n",
    "    model = AccentClassifier().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        for i, (inputs, labels) in enumerate(trainloader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.to(device))\n",
    "            loss = nn.BCELoss()(outputs,labels.to(device).reshape(-1,1))\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            outputs = outputs.reshape(1, -1)\n",
    "            outputs = outputs.squeeze()\n",
    "            for i in range(outputs.size()[0]):\n",
    "                if (labels[i] == 0 and outputs[i] < 0.5) or (labels[i] == 1 and outputs[i] >= 0.5):\n",
    "                    correct += 1\n",
    "\n",
    "       \n",
    "        print(f\"Epoch {epoch + 1}  Loss: {running_loss / len(trainloader)}  Accuracy: {100 * correct / len(train_ids)}\")\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_correct = 0\n",
    "            for j, (d,l) in enumerate(testloader):\n",
    "                o = model(d.to(device))\n",
    "                loss = nn.BCELoss()(o,l.to(device).reshape(-1,1))\n",
    "                test_loss += loss.item()\n",
    "                o = o.reshape(1,-1)\n",
    "                o = o.squeeze()\n",
    "                for i in range(o.size()[0]):\n",
    "                    if (l[i] == 0 and o[i] < 0.5) or (l[i] == 1 and o[i] >= 0.5):\n",
    "                        test_correct += 1\n",
    "\n",
    "            accuracy = 100 * test_correct / len(test_ids)\n",
    "            \n",
    "    print(f\"Model test accuracy for fold {fold}: {accuracy} \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.85009861932939\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_data,batch_size=32,shuffle=True)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    for j, (d,l) in enumerate(test_loader):\n",
    "        o = model(d.to(device))\n",
    "        loss = nn.BCELoss()(o,l.to(device).reshape(-1,1))\n",
    "        val_loss += loss.item()\n",
    "        o = o.reshape(1,-1)\n",
    "        o = o.squeeze()\n",
    "        for i in range(o.size()[0]):\n",
    "            if (l[i] == 0 and o[i] < 0.5) or (l[i] == 1 and o[i] >= 0.5):\n",
    "                test_correct += 1\n",
    "\n",
    "    accuracy = 100 * test_correct / len(test_data)\n",
    "            \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifying specific set of audio samples\n",
    "def predict(test_dir):\n",
    "    \n",
    "    predictions = dict()\n",
    "    for f in os.listdir(test_dir):\n",
    "        audio_chunks = segment_and_standardize_audio(test_dir / f, 1000)\n",
    "        num_american_pred = 0\n",
    "        for seg in audio_chunks:\n",
    "\n",
    "            samples = seg.get_array_of_samples()\n",
    "            arr = np.array(samples).astype(np.float32)/32768 # 16 bit \n",
    "            arr = librosa.core.resample(arr, seg.frame_rate, 22050, res_type='kaiser_best') \n",
    "\n",
    "            mfcc = librosa.feature.mfcc(y=arr, sr=22050, n_mfcc=50)\n",
    "            data = generate_mfcc_data(mfcc)\n",
    "            pred = model(torch.from_numpy(data).unsqueeze(0).float().to(device)).item()\n",
    "            if pred > 0.5:\n",
    "                num_american_pred += 1\n",
    "        \n",
    "        frac_american_preds = num_american_pred / len(audio_chunks)\n",
    "        \n",
    "        if frac_american_preds >= 0.5:\n",
    "            predictions[f] = 1\n",
    "        else:\n",
    "            predictions[f] = 0\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    x = torch.randn(1, 3, 50, 44, requires_grad=True).to(device)\n",
    "    torch.save(model.state_dict(), \"binary_accent_classifier.pt\")\n",
    "    torch.onnx.export(model, x, \"binary_accent_classifier.onnx\", opset_version=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
